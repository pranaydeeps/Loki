{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "featured-settlement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ab8f536e1a48cd24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/artemis/.cache/huggingface/datasets/csv/default-ab8f536e1a48cd24/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/artemis/.cache/huggingface/datasets/csv/default-ab8f536e1a48cd24/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23bb8e5be9d4f50953e345b80d8fca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "from torch import nn\n",
    "\n",
    "tag2idx = {'hero' : 0, 'villain': 1, 'victim': 2, 'other': 3}\n",
    "idx2tag = {0 : 'hero', 1: 'villain', 2: 'victim', 3: 'other'}\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")\n",
    "\n",
    "ending_names = [\"is a hero\", \"is a villain\", \"is a victim\", \"is neutral\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sentence\"]]\n",
    "    question_headers = examples[\"aspect\"]\n",
    "    second_sentences = [\n",
    "     [f\"{header} {end}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "    #print(second_sentences)\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"csv\", data_files = {\"test\": \"final_testdata.csv\"})\n",
    "tokenized_data = data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_output = trainer.predict(tokenized_data[\"test\"])\n",
    "\n",
    "test_csv = []\n",
    "for i, point in enumerate(tokenized_data[\"test\"]):\n",
    "    hero, villain, victim, other = torch.softmax(torch.tensor(train_output[0][i]),dim=0).numpy()\n",
    "    #print((hero,villain,victim,other))\n",
    "    train_csv.append([point[\"sentence\"], point[\"aspect\"], point[\"image\"], float(hero), float(villain), float(victim), float(other)])\n",
    "\n",
    "with open('testfinal_with_logits.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(train_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
